{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_preprocessed_ds(cfg, sub_cfg):\n",
    "    ds_hash = md5(yaml.dump(sub_cfg, Dumper=yaml.Dumper))\n",
    "    prepared_ds_path = _get_path(ds_hash, cfg)\n",
    "    dataset = None\n",
    "\n",
    "    # pylint: disable=duplicate-code\n",
    "    if (\n",
    "        cfg.dataset_prepared_path\n",
    "        and any(prepared_ds_path.glob(\"*\"))\n",
    "        and not cfg.is_preprocess\n",
    "    ):\n",
    "        LOG.info(f\"Loading prepared dataset from disk at {prepared_ds_path}...\")\n",
    "        dataset = load_from_disk(str(prepared_ds_path))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_prepare_dpo_datasets(cfg):\n",
    "    def load_split(dataset_cfgs, _cfg):\n",
    "        split_datasets: List[Any] = []\n",
    "        for i, ds_cfg in enumerate(dataset_cfgs):\n",
    "            if ds_cfg[\"ds_type\"] == \"json\":\n",
    "                print(ds_cfg)\n",
    "                for data_file in ds_cfg[\"data_files\"]:\n",
    "                    data_files = {ds_cfg[\"split\"]: data_file}\n",
    "                    print(data_files)\n",
    "                    ds = load_dataset(  # pylint: disable=invalid-name\n",
    "                        \"json\",\n",
    "                        data_files=data_files,\n",
    "                        split=ds_cfg[\"split\"],\n",
    "                    )\n",
    "                    split_datasets.insert(i, ds)\n",
    "            else:\n",
    "                ds = load_dataset(  # pylint: disable=invalid-name\n",
    "                    ds_cfg[\"path\"],\n",
    "                    split=ds_cfg[\"split\"],\n",
    "                )\n",
    "                split_datasets.insert(i, ds)\n",
    "\n",
    "        tokenizer = None\n",
    "\n",
    "        for i, data_set in enumerate(split_datasets):\n",
    "            _type = dataset_cfgs[i][\"type\"]\n",
    "            if _type:\n",
    "                if isinstance(_type, DictDefault):\n",
    "                    _type = \"user_defined.default\"\n",
    "                if _cfg.rl == \"orpo\":\n",
    "                    ds_transform_fn = load_orpo(_type, _cfg, dataset_idx=i)\n",
    "                elif _cfg.rl == \"kto\":\n",
    "                    ds_transform_fn = load_kto(_type, _cfg, dataset_idx=i)\n",
    "                else:\n",
    "                    print(\"load_dpo\")\n",
    "                    print(_type)\n",
    "                    print(_cfg)\n",
    "                    ds_transform_fn = load_dpo(_type, _cfg, dataset_idx=i)\n",
    "\n",
    "                print(ds_transform_fn)\n",
    "                split_datasets[i] = map_dataset(\n",
    "                    cfg, data_set, ds_transform_fn, tokenizer\n",
    "                )\n",
    "            elif _cfg.rl == \"kto\":\n",
    "                ds_transform_fn = load_kto(_type, _cfg, dataset_idx=i)\n",
    "                split_datasets[i] = map_dataset(\n",
    "                    cfg, data_set, ds_transform_fn, tokenizer\n",
    "                )\n",
    "            else:\n",
    "                # If no `type` is provided, assume the dataset is already in the expected format with\n",
    "                # \"prompt\", \"chosen\" and \"rejected\" already preprocessed\n",
    "                split_datasets[i] = data_set\n",
    "\n",
    "        return concatenate_datasets(split_datasets)\n",
    "\n",
    "#     with zero_first(is_main_process()):\n",
    "    train_is_preprocessed = False\n",
    "    eval_is_preprocessed = False\n",
    "    if train_dataset := _load_preprocessed_ds(cfg, cfg.datasets):\n",
    "        train_is_preprocessed = True\n",
    "    else:\n",
    "        train_dataset = load_split(cfg.datasets, cfg)\n",
    "\n",
    "    eval_dataset = None\n",
    "    if cfg.test_datasets:\n",
    "        if eval_dataset := _load_preprocessed_ds(cfg, cfg.test_datasets):\n",
    "            eval_is_preprocessed = True\n",
    "        else:\n",
    "            eval_dataset = load_split(cfg.test_datasets, cfg)\n",
    "    if not eval_dataset:\n",
    "        eval_dataset = None\n",
    "        \n",
    "    print(train_dataset)\n",
    "\n",
    "    if not train_is_preprocessed:\n",
    "        _save_preprocessed_ds(cfg, cfg.datasets, train_dataset)\n",
    "    if eval_dataset and not eval_is_preprocessed:\n",
    "        _save_preprocessed_ds(cfg, cfg.test_datasets, eval_dataset)\n",
    "\n",
    "    return train_dataset, eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b06ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_dataset(cfg, data_set, ds_transform_fn, tokenizer):\n",
    "    sig = inspect.signature(ds_transform_fn)\n",
    "    if \"tokenizer\" in sig.parameters:\n",
    "        if not tokenizer:\n",
    "            tokenizer = load_tokenizer(cfg)\n",
    "        ds_transform_fn = partial(ds_transform_fn, tokenizer=tokenizer)\n",
    "\n",
    "    data_set = data_set.map(\n",
    "        ds_transform_fn,\n",
    "        desc=\"Mapping RL Dataset\",\n",
    "    )\n",
    "    if isinstance(data_set, DatasetDict):\n",
    "        data_set = data_set[\"train\"]\n",
    "    return data_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c672c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data handling helpers\"\"\"\n",
    "\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def md5(to_hash: str, encoding: str = \"utf-8\") -> str:\n",
    "    try:\n",
    "        return hashlib.md5(to_hash.encode(encoding), usedforsecurity=False).hexdigest()\n",
    "    except TypeError:\n",
    "        return hashlib.md5(to_hash.encode(encoding)).hexdigest()  # nosec\n",
    "\n",
    "    \n",
    "def _get_path(ds_hash, cfg):\n",
    "    prepared_ds_path = (\n",
    "        Path(cfg.dataset_prepared_path) / ds_hash\n",
    "        if cfg.dataset_prepared_path\n",
    "        else Path(DEFAULT_DATASET_PREPARED_PATH) / ds_hash\n",
    "    )\n",
    "\n",
    "    return prepared_ds_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2accdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "import yaml\n",
    "\n",
    "\n",
    "class DictDefault(Dict):\n",
    "    \"\"\"\n",
    "    A Dict that returns None instead of returning empty Dict for missing keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        return None\n",
    "\n",
    "    def __or__(self, other):\n",
    "        return DictDefault(super().__ror__(other))\n",
    "\n",
    "    \n",
    "from pathlib import Path\n",
    "DEFAULT_DATASET_PREPARED_PATH = \"last_run_prepared\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb335ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9d646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logging\n",
    "\n",
    "LOG = logging.getLogger(\"axolotl\")\n",
    "\n",
    "\n",
    "def load(strategy, cfg, module_base=None, **kwargs):\n",
    "    try:\n",
    "        load_fn = strategy.split(\".\")[-1]\n",
    "        strategy = \".\".join(strategy.split(\".\")[:-1])\n",
    "        mod = importlib.import_module(f\".{strategy}\", module_base)\n",
    "        func = getattr(mod, load_fn)\n",
    "        return func(cfg, **kwargs)\n",
    "    except Exception:  # pylint: disable=broad-exception-caught\n",
    "        LOG.warning(f\"unable to load strategy {strategy}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122dca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(cfg, dataset_idx=0, **kwargs):  # pylint: disable=unused-argument\n",
    "    ds_cfg = cfg[\"datasets\"][dataset_idx][\"type\"]\n",
    "    if not isinstance(ds_cfg, dict):\n",
    "        raise ValueError(\n",
    "            f\"User-defined dataset type must be a dictionary. Got: {ds_cfg}\"\n",
    "        )\n",
    "    field_prompt = ds_cfg.get(\"field_prompt\", \"prompt\")\n",
    "    field_system = ds_cfg.get(\"field_system\", \"system\")\n",
    "    field_chosen = ds_cfg.get(\"field_chosen\", \"chosen\")\n",
    "    field_rejected = ds_cfg.get(\"field_rejected\", \"rejected\")\n",
    "    prompt_format = ds_cfg.get(\"prompt_format\")\n",
    "    if not prompt_format:\n",
    "        prompt_format = \"{\" + field_prompt + \"}\"\n",
    "    chosen_format = ds_cfg.get(\"chosen_format\")\n",
    "    if not chosen_format:\n",
    "        chosen_format = \"{\" + field_chosen + \"}\"\n",
    "    rejected_format = ds_cfg.get(\"rejected_format\")\n",
    "    if not rejected_format:\n",
    "        rejected_format = \"{\" + field_rejected + \"}\"\n",
    "\n",
    "    def transform_fn(sample):\n",
    "        if (\n",
    "            \"{\" + field_system + \"}\" in prompt_format\n",
    "            and field_system in sample\n",
    "            and sample[field_system]\n",
    "        ):\n",
    "            sample[\"prompt\"] = prompt_format.format(\n",
    "                system=sample[field_system], prompt=sample[field_prompt]\n",
    "            )\n",
    "        else:\n",
    "            sample[\"prompt\"] = prompt_format.format(prompt=sample[\"prompt\"])\n",
    "        sample[\"chosen\"] = chosen_format.format(chosen=sample[field_chosen])\n",
    "        sample[\"rejected\"] = rejected_format.format(rejected=sample[field_rejected])\n",
    "        return sample\n",
    "\n",
    "    return transform_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d178ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from axolotl.prompt_strategies.dpo import load as load_dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f5751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "module for base dataset transform strategies\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "import logging\n",
    "\n",
    "LOG = logging.getLogger(\"axolotl\")\n",
    "\n",
    "\n",
    "def load(strategy, cfg, module_base=None, **kwargs):\n",
    "    try:\n",
    "        load_fn = strategy.split(\".\")[-1]\n",
    "        strategy = \".\".join(strategy.split(\".\")[:-1])\n",
    "        print(\"strategy\", strategy)\n",
    "        print(\"module_base\", module_base)\n",
    "        mod = importlib.import_module(f\".{strategy}\", module_base)\n",
    "        func = getattr(mod, load_fn)\n",
    "        return func(cfg, **kwargs)\n",
    "    except Exception:  # pylint: disable=broad-exception-caught\n",
    "        LOG.warning(f\"unable to load strategy {strategy}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068ed68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94555b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'axolotl.prompt_strategies.dpo.user_defined' from '/Users/htong/Desktop/tmpsrc/axolotl/src/axolotl/prompt_strategies/dpo/user_defined.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.import_module(\".user_defined\", \"axolotl.prompt_strategies.dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73570151",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DictDefault()\n",
    "cfg[\"datasets\"] = [{\"ds_type\": \"json\", \"split\": \"train\", \"type\": {},\n",
    "                    \"data_files\": [\"finetuning_qwen2_dpo.jsonl\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a946b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function axolotl.prompt_strategies.dpo.user_defined.default.<locals>.transform_fn(sample)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = importlib.import_module(\".user_defined\", \"axolotl.prompt_strategies.dpo\")\n",
    "func = getattr(mod, \"default\")\n",
    "func(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86cab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ds_type': 'json', 'split': 'train', 'type': {}, 'data_files': ['finetuning_qwen2_dpo.jsonl']}\n",
      "{'train': 'finetuning_qwen2_dpo.jsonl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a118732d307f4487a78c50ab8b899a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 208\n",
      "})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_save_preprocessed_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_prepare_dpo_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mload_prepare_dpo_datasets\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_is_preprocessed:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43m_save_preprocessed_ds\u001b[49m(cfg, cfg\u001b[38;5;241m.\u001b[39mdatasets, train_dataset)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m eval_is_preprocessed:\n\u001b[1;32m     94\u001b[0m     _save_preprocessed_ds(cfg, cfg\u001b[38;5;241m.\u001b[39mtest_datasets, eval_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_save_preprocessed_ds' is not defined"
     ]
    }
   ],
   "source": [
    "load_prepare_dpo_datasets(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db8627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0405a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
